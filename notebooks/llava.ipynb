{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177a465a-b582-49af-b8b7-3bd2a433459d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from PIL import Image\n",
    "import requests\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoProcessor, LlavaForConditionalGeneration, Blip2Model, Blip2Processor, Blip2ForConditionalGeneration, Blip2Config\n",
    "import time\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "from transformers import TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import wandb\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "print(transformers.__version__)\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990fbcf-d84e-4002-beb1-db5846145fc3",
   "metadata": {},
   "source": [
    "# Download Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856aeca-eef1-41dd-b6aa-57e0c3fe9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True,\n",
    "    # load_in_4bit=True\n",
    ").to(device)\n",
    "print(f'Memory Allocated after instantiating models: {torch.cuda.memory_allocated(0)/1e9:.4g} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a292a-f910-4083-b45f-1cfc1a0a7194",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8b5d1-dddd-4d22-a191-4a75b53d94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./test_trainer',     # output directory\n",
    "    num_train_epochs=2,              # total # of training epochs\n",
    "    per_device_train_batch_size=5,   # batch size per device during training\n",
    "    per_device_eval_batch_size=5,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"coffee-bot\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"architecture\": model.__class__.__name__,\n",
    "        \"dataset\": \"ROASTME-9540\",\n",
    "        \"num_train_epoch\": training_args.num_train_epochs,\n",
    "        \"warmup_steps\": training_args.warmup_steps,\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"per_device_eval_batch_size\": training_args.per_device_eval_batch_size,        \n",
    "    }\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, eps=10e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f45e2f7c-b597-4e00-a066-9f26841a71c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     18\u001b[0m label_input_ids \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m output\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/coffeebot-p3lKt8zM-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/coffeebot-p3lKt8zM-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/coffeebot-p3lKt8zM-py3.10/lib/python3.10/site-packages/transformers/models/llava/modeling_llava.py:431\u001b[0m, in \u001b[0;36mLlavaForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected select feature strategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvision_feature_select_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m     )\n\u001b[1;32m    430\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_modal_projector(selected_image_feature)\n\u001b[0;32m--> 431\u001b[0m inputs_embeds, attention_mask, labels, position_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_input_ids_with_image_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull_like(attention_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mignore_index)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/coffeebot-p3lKt8zM-py3.10/lib/python3.10/site-packages/transformers/models/llava/modeling_llava.py:333\u001b[0m, in \u001b[0;36mLlavaForConditionalGeneration._merge_input_ids_with_image_features\u001b[0;34m(self, image_features, inputs_embeds, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    330\u001b[0m image_to_overwrite \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mall(final_embedding \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    331\u001b[0m image_to_overwrite \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m image_to_overwrite\u001b[38;5;241m.\u001b[39mcumsum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m nb_image_pad[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mto(target_device)\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_to_overwrite\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m!=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel():\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input provided to the model are wrong. The number of image tokens is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39msum(special_image_token_mask)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m while\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the number of image given to the model is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_images\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This prevents correct indexing and breaks batch generation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     )\n\u001b[1;32m    339\u001b[0m final_embedding[image_to_overwrite] \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, embed_dim)\u001b[38;5;241m.\u001b[39mto(target_device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<image>\\nUSER: What's the content of the image?\\nASSISTANT:\"\n",
    "# url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "url = \"https://i.redd.it/0o673i8z5bkb1.jpg\"\n",
    "label = \"I failed a drug test just by looking at this picture.\"\n",
    "raw_image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "\n",
    "inputs = processor(prompt, raw_image, return_tensors='pt')\n",
    "labels = processor.tokenizer(label, return_tensors=\"pt\")\n",
    "\n",
    "# output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "# print(processor.decode(output[0][2:], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "pixel_values = inputs.pixel_values.to(device, dtype=torch.float16)\n",
    "input_ids = inputs.input_ids.to(device, dtype=torch.long)\n",
    "attention_mask = inputs.attention_mask.to(device, dtype=torch.long)\n",
    "label_input_ids = labels.input_ids.to(device, dtype=torch.long)\n",
    "\n",
    "output = model(\n",
    "    pixel_values=pixel_values,\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=label_input_ids,\n",
    ")\n",
    "output.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a0afb6-4f2e-40e9-8c66-7015b8c42630",
   "metadata": {},
   "source": [
    "# Read in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838faeb-a521-4c83-967f-f7d789290e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../datasets/full_ds.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08db58-5ee2-4800-a265-93d1425f9c96",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f40e0-9bb1-4c96-823d-0b87177be2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_on_row(index):\n",
    "    row = df.iloc[index]\n",
    "    label = row['comment']\n",
    "    prompt = \"USER: <image>\\nHow would you describe this person?\\nASSISTANT:\"\n",
    "    image_url = row['image_url']\n",
    "    image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "    display(image)\n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\", padding=True, max_length=256, truncation=True).to(device, torch.float16)\n",
    "\n",
    "    # ids = reference_model.generate(**inputs)\n",
    "    # print(f'REFERENCE: {processor.batch_decode(ids, skip_special_tokens=True)[0].strip()}')\n",
    "    print(f'LABEL: {label}')\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "    return processor.decode(generated_ids[0][2:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968cf489-54ed-4745-bc24-a8eac94e6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference_on_row(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a87af3-4d8b-4862-8fd3-79f107ebfc15",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a749c96-cdfd-46f8-9ec0-e5eb57b478dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model.train()\n",
    "\n",
    "# Dataset variables\n",
    "prompt = \"USER: <image>\\nHow would you describe this person?\\nASSISTANT:\"\n",
    "labels = []\n",
    "image = None\n",
    "submission_id = None\n",
    "out = None\n",
    "losses = []\n",
    "\n",
    "\n",
    "with torch.autograd.detect_anomaly():\n",
    "    for epoch in range(training_args.num_train_epochs):\n",
    "        print(f'EPOCH #{epoch}')\n",
    "        for index, row in df.iterrows():    \n",
    "            comment = row['comment']\n",
    "            image_url = row['image_url']\n",
    "            \n",
    "            if submission_id is None:\n",
    "                submission_id = row['submission_id']\n",
    "        \n",
    "            if image is None:\n",
    "                image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "        \n",
    "            # We reached a new submission\n",
    "            if row['submission_id'] != submission_id or len(labels) >= training_args.per_device_train_batch_size:        \n",
    "                # Expand the inputs\n",
    "                prompts = [prompt] * len(labels)\n",
    "                image_inputs = [image] * len(labels)\n",
    "    \n",
    "                try:\n",
    "                    \n",
    "                    # Forward Pass \n",
    "                    print(f'Forward pass {len(prompts)} prompts and {len(labels)} labels ({row[\"submission_id\"]})')\n",
    "                    label_input_ids = processor.tokenizer(labels, return_tensors=\"pt\", padding=True, max_length=256, truncation=True).input_ids.to(device, torch.float16)\n",
    "\n",
    "                    print(inputs)\n",
    "                    print(label_input_ids)\n",
    "                    inputs = processor(text=prompt, images=image, return_tensors=\"pt\", padding=True, max_length=256, truncation=True).to(device)\n",
    "                \n",
    "                    out = model(**inputs, labels=label_input_ids)\n",
    "                    print(out.loss)\n",
    "        \n",
    "                    # Backprop (calculate gradients)\n",
    "                    out.loss.backward()\n",
    "        \n",
    "                    # Update weights using gradients\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Reset the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    print(f\"Loss at step {index} = {out.loss.item()}\")\n",
    "    \n",
    "                    losses.append(out.loss.item())\n",
    "                    print(f'Memory Allocated after first pass : {torch.cuda.memory_allocated(0)/1e9:.4g} GB')\n",
    "                    print(\"--------------\")\n",
    "                    \n",
    "                    # Reset\n",
    "                    submission_id = row['submission_id']\n",
    "                    image = Image.open(requests.get(row['image_url'], stream=True).raw)\n",
    "                    labels = [row['comment']]\n",
    "                except ValueError as ve:\n",
    "                    # Code to handle the ValueError\n",
    "                    print(f\"Error: {ve}\")\n",
    "                    print(f\"INPUTS: {prompts}\")\n",
    "                    print(f\"LABELS: {labels}\")\n",
    "                    break\n",
    "            else:\n",
    "                labels.append(comment)\n",
    "\n",
    "model.save_pretrained(\"./llava_fine_tuned_model\", from_pt=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041eb0e6-c61b-4780-a588-b52abe0ceb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(row['comment'])\n",
    "    if index > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b81e31c-e17d-4df6-9535-c713219be895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 336, 336])\n",
      "torch.Size([2, 9])\n",
      "torch.Size([2, 9])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 32\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(attention_mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# model(\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     pixel_values=pixel_values,\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#     input_ids=input_ids,\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     attention_mask=attention_mask,\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     labels=input_ids,\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ).loss\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[1;32m     34\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlavaForConditionalGeneration\n",
    "# model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "pixel_values = torch.randn(\n",
    "    (2, 3, 336, 336),\n",
    "    dtype=torch.float16\n",
    ").to(device)\n",
    "input_ids = torch.tensor(\n",
    "    [\n",
    "        [32001, 32001, 1, 15043,  7084, 32000, 29871,    13, 7900],\n",
    "        [1, 15043,  7084, 29901, 29871, 32000, 29871,    13, 7900]\n",
    "    ], dtype=torch.long\n",
    ").to(device)\n",
    "attention_mask = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "    ], dtype=torch.long\n",
    ").to(device)\n",
    "\n",
    "print(pixel_values.shape)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "# model(\n",
    "#     pixel_values=pixel_values,\n",
    "#     input_ids=input_ids,\n",
    "#     attention_mask=attention_mask,\n",
    "#     labels=input_ids,\n",
    "# ).loss\n",
    "\n",
    "model.cpu()\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_cached()\n",
    "\n",
    "print(f'Memory Allocated after instantiating models: {torch.cuda.memory_allocated(0)/1e9:.4g} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d79932-31ce-4ad9-b762-e7b422e43784",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f44269e-d3b1-4646-92cf-0714f77d2e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb  6 21:19:58 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   28C    P0    48W / 400W |  26669MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2196390      C   ...3lKt8zM-py3.10/bin/python    26666MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c2e19-e247-4805-aa15-582abc1d6bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
