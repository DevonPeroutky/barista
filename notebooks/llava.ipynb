{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47656296-dfdc-4e99-9849-80e5ee7c7082",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cf9fccd-d31a-4cab-9b5f-bda5f1428473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoProcessor, LlavaForConditionalGeneration, Blip2Model, Blip2Processor, Blip2ForConditionalGeneration, Blip2Config\n",
    "import time\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11913884-90c6-4233-9609-f9a8b6e1a658",
   "metadata": {},
   "source": [
    "# Load Model Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06824f3c-3586-4c60-84c1-e1323910d2de",
   "metadata": {},
   "source": [
    "## LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7a25a-4e5f-4cee-a66c-b4b692b30d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True,).to(0)\n",
    "# processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ca617-640b-4952-b30d-d2c413d05da7",
   "metadata": {},
   "source": [
    "## BLIP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "134d804e-6bf2-45e2-9fa1-d56a0154336e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b39d803ef144ba8b979fe6ba4c40f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devonperoutky/.cache/pypoetry/virtualenvs/coffeebot-p3lKt8zM-py3.10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Download BLIP2 weights\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d361039-bc38-4f8f-8e98-2530e4acd478",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc909af5-7e98-4927-823b-0bc0ee17aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<image>\\nUSER: What is in the image?\\nASSISTANT:\"\n",
    "label_text = '</s> The image is of two cats laying on a couch with remotes on the couch\\n'\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b974c57-a7fe-404c-bab5-904deb2a291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6110c-8b90-45e9-b23a-ac1a4d66c0f8",
   "metadata": {},
   "source": [
    "## One loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d0afc-4689-4033-9b22-a29faee131a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = ['</s> The image is of two cats laying on a couch with remotes on the couch\\n', ' The image is of two cats laying on a couch with remotes on the couch\\n', '</s>The image is of two cats laying on a couch with remotes on the couch\\n', 'Swagger', ' The', '</s> The']\n",
    "\n",
    "for label_text in yb: \n",
    "    inputs = processor(image, prompt, return_tensors=\"pt\").to(0, torch.float16)\n",
    "    label_input_ids = processor.tokenizer.encode(label_text, return_tensors=\"pt\")\n",
    "    print(inputs.input_ids.shape)\n",
    "    generated_ids = model(pixel_values=inputs.pixel_values, input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=label_input_ids)\n",
    "\n",
    "    final_logit_layer = generated_ids.logits[:,-1,:]\n",
    "    max_token_prob = F.softmax(final_logit_layer).argmax()\n",
    "    prediction = processor.batch_decode(max_token_prob.unsqueeze(0), skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    print(f'Prediction {prediction} vs. {label_text} has loss: {generated_ids.loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92982499-f0bd-42eb-bc78-7130831aa282",
   "metadata": {},
   "source": [
    "## Matrix loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca07a04d-a01c-404d-a579-808d132a6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Question: What is in the image? Answer:\"\n",
    "label_texts = [\n",
    "    '</s> The image is of two cats laying on a couch with remotes on the couch\\n',\n",
    "    ' The image is of two cats laying on a couch with remotes on the couch\\n',\n",
    "    'Swagger',\n",
    "    ' The',\n",
    "    '</s> The'\n",
    "][:3]\n",
    "\n",
    "tokens = [processor.tokenizer.encode(s, return_tensors=\"pt\", padding=\"max_length\", max_length=512) for s in label_texts]\n",
    "print(label_texts)\n",
    "print(type(label_texts))\n",
    "# processor.tokenizer.encode(label_texts, return_tensors=\"pt\", padding=\"max_length\", max_length=512)\n",
    "processor.tokenizer(label_texts, return_tensors=\"pt\", padding=\"max_length\", max_length=512).input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614c373-b5e5-4c0f-bc87-2a9126792ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "# !nvidia-smi --gpu-reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065b2ad-be34-47ef-be04-5e22955412b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU\n",
    "torch.cuda.empty_cache()\n",
    "del generated_ids\n",
    "del model\n",
    "del processor\n",
    "del forward_ids\n",
    "del final_layer_logits\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85401876-672b-43d8-aeb3-9c0fe9bcada2",
   "metadata": {},
   "source": [
    "# Test Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0fa79-8bfd-4dfd-b097-aaf5b35bdcef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(0, torch.float16)\n",
    "forward_ids = model(pixel_values=inputs.pixel_values, input_ids=inputs.input_ids, attention_mask=inputs.attention_mask)\n",
    "# generated_ids = model.generate(pixel_values=inputs.pixel_values, input_ids=inputs.input_ids, max_length=200, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10a9f8-b157-4863-83ef-3e5342e5b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer_logits = forward_ids.logits[:, -1, :]\n",
    "probs = F.softmax(final_layer_logits)\n",
    "idx = torch.argmax(probs)\n",
    "processor.decode(idx)\n",
    "\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42327f4-427d-432e-bf03-c22269226928",
   "metadata": {},
   "source": [
    "# Training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8b70a-bb81-444f-b711-93a7f35233cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40206e5-c160-42d2-b1b3-cb921486ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./test_trainer',     # output directory\n",
    "    num_train_epochs=2,              # total # of training epochs\n",
    "    per_device_train_batch_size=4,   # batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f91674-a1e6-46da-9bdc-5075c1b8acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args.num_train_epochs\n",
    "model.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13cdbee-d432-42cf-8800-30fcfaf79632",
   "metadata": {},
   "source": [
    "# TO-DOs\n",
    "\n",
    "- [] Compute Metrics. Are we just using NTP loss? AKA cross entropy\n",
    "- [] Tune/expose hyperparameters for configuration\n",
    "- [] Setup with W&Bs\n",
    "- [] Build training and eval datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c11d915-beff-487b-b340-3d2e623a345f",
   "metadata": {},
   "source": [
    "# W & B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75baaebe-7329-4ca1-8e98-71c3f31e6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already handled\n",
    "# !wandb login\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"coffee-bot\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"architecture\": model.__class__.__name__,\n",
    "        \"dataset\": \"CIFAR-100\",\n",
    "        \"num_train_epoch\": training_args.num_train_epochs,\n",
    "        \"warmup_steps\": training_args.warmup_steps,\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"per_device_eval_batch_size\": training_args.per_device_eval_batch_size,        \n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df16060-e0d0-4551-adca-300e365a41f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54580559-1e86-404e-bff1-e8c578d4b991",
   "metadata": {},
   "source": [
    "# I'M NOT CRAZY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1975797-92a8-4451-b4bc-6728f94190a7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(0, torch.float16)\n",
    "print(inputs.input_ids)\n",
    "\n",
    "test = torch.tensor([[81.0]]).to(0, torch.float16)\n",
    "\n",
    "inputs.input_ids = inputs.input_ids.to(torch.float16)\n",
    "\n",
    "print(test)\n",
    "print(inputs.input_ids)\n",
    "torch.cat((inputs.input_ids, test), dim=1)\n",
    "# torch.cat((inputs.input_ids, test, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94943f38-77c5-4d7e-9fca-10e92236d243",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df715d-53b7-42be-a8c0-f1bba299e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = None\n",
    "idx = None\n",
    "curr_prompt = prompt\n",
    "input_ids = None\n",
    "res_ids = None\n",
    "\n",
    "# While pred != '\\n'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Get image_embedding and project\n",
    "input_ids, language_model_inputs, vision_outputs = image_encoding(model=model, processor=processor, curr_prompt=curr_prompt)\n",
    "\n",
    "while idx != 50118:\n",
    "\n",
    "    # Forward pass\n",
    "    forward_ids = forward_from_image_query_output(model=model, language_model_inputs=language_model_inputs, input_ids=input_ids)\n",
    "\n",
    "    # Get the final layer of logits\n",
    "    final_layer_logits = forward_ids['logits'][:, -1, :]\n",
    "\n",
    "    # Get prediction\n",
    "    probs = F.softmax(final_layer_logits)\n",
    "    idx = torch.argmax(probs)\n",
    "\n",
    "    # Convert to shape\n",
    "    idx = torch.tensor([[idx.item()]]).to('cuda')\n",
    "\n",
    "    if res_ids is None:\n",
    "        res_ids = idx\n",
    "    else:\n",
    "        res_ids = torch.cat([res_ids, idx], dim = 1)\n",
    "        \n",
    "    input_ids = torch.cat([input_ids, idx], dim = 1)\n",
    "    # print(processor.batch_decode(res_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "\n",
    "end_time = time.time()\n",
    "print(res_ids)\n",
    "print(f'Took {end_time - start_time} seconds')\n",
    "processor.batch_decode(input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563466df-d00e-4933-ac99-64e6bedc0acf",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cecdfce-7b75-4f47-8170-92de28fe3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_from_image_query_output(\n",
    "    model,\n",
    "    language_model_inputs: torch.FloatTensor,\n",
    "    input_ids: torch.FloatTensor,\n",
    "    attention_mask: Optional[torch.LongTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "):\n",
    "    output_attentions = None\n",
    "    output_hidden_states = None\n",
    "    return_dict = model.config.use_return_dict\n",
    "    \n",
    "    # step 3: use the language model, conditioned on the query outputs and the prompt\n",
    "    language_model_attention_mask = torch.ones(\n",
    "        language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n",
    "    )\n",
    "    inputs_embeds = model.language_model.get_input_embeddings()(input_ids)\n",
    "    inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "    expected_device = language_model_attention_mask.device\n",
    "    attention_mask = torch.cat([language_model_attention_mask, attention_mask.to(expected_device)], dim=1)\n",
    "\n",
    "    outputs = model.language_model(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    logits = outputs.logits if return_dict else outputs[0]\n",
    "    loss = None\n",
    "    # we compute the loss here since we need to take into account the sequence length of the query embeds\n",
    "    if labels is not None:\n",
    "        labels = labels.to(logits.device)\n",
    "        logits = logits[:, -labels.size(1) :, :]\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous().to(logits.device)\n",
    "\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "        loss = loss_fct(shift_logits.view(-1, model.config.text_config.vocab_size), shift_labels.view(-1))\n",
    "\n",
    "    if not return_dict:\n",
    "        output = (logits, outputs)\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'logits': logits,\n",
    "        'language_model_outputs': outputs,\n",
    "    }\n",
    "\n",
    "def image_encoding(\n",
    "    model,\n",
    "    processor,\n",
    "    image,\n",
    "    curr_prompt\n",
    "):  \n",
    "    inputs = processor(image, curr_prompt, return_tensors=\"pt\").to(0, torch.float16)\n",
    "    print(inputs)\n",
    "    pixel_values = inputs.pixel_values\n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    decoder_input_ids = None\n",
    "    decoder_attention_mask = None\n",
    "    output_attentions = None\n",
    "    output_hidden_states = None\n",
    "    labels = None\n",
    "    return_dict = model.config.use_return_dict\n",
    "\n",
    "    # step 1: forward the images through the vision encoder,\n",
    "    # to get image embeddings of shape (batch_size, seq_len, hidden_size)\n",
    "    vision_outputs = model.vision_model(\n",
    "        pixel_values=pixel_values,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    image_embeds = vision_outputs[0]\n",
    "    print(f'Image Embedds: {image_embeds.shape}')\n",
    "\n",
    "    # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n",
    "    image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n",
    "\n",
    "    query_tokens = model.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "    query_outputs = model.qformer(\n",
    "        query_embeds=query_tokens,\n",
    "        encoder_hidden_states=image_embeds,\n",
    "        encoder_attention_mask=image_attention_mask,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    query_output = query_outputs[0]\n",
    "\n",
    "    # step 3: use the language model, conditioned on the query outputs and the prompt\n",
    "    return input_ids, model.language_projection(query_output), vision_outputs\n",
    "\n",
    "def custom_forward(\n",
    "    model,\n",
    "    pixel_values: torch.FloatTensor,\n",
    "    input_ids: torch.FloatTensor,\n",
    "    attention_mask: Optional[torch.LongTensor] = None,\n",
    "    decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "    decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    "):\n",
    "    vision_outputs = model.vision_model(\n",
    "        pixel_values=pixel_values,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    image_embeds = vision_outputs[0]\n",
    "    print(f'Image Embeds: {image_embeds.shape}')\n",
    "\n",
    "    # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n",
    "    image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n",
    "\n",
    "    query_tokens = model.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "    query_outputs = model.qformer(\n",
    "        query_embeds=query_tokens,\n",
    "        encoder_hidden_states=image_embeds,\n",
    "        encoder_attention_mask=image_attention_mask,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    query_output = query_outputs[0]\n",
    "\n",
    "    print(f'Query Output: {query_output.shape}')\n",
    "\n",
    "    # step 3: use the language model, conditioned on the query outputs and the prompt\n",
    "    language_model_inputs = model.language_projection(query_output)\n",
    "    language_model_attention_mask = torch.ones(\n",
    "        language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n",
    "    )\n",
    "    print(f'Input Ids: {input_ids.shape}')\n",
    "    inputs_embeds = model.language_model.get_input_embeddings()(input_ids)\n",
    "    print(f'Input Embeds: {inputs_embeds.shape}')\n",
    "    print(f'language_model_inputs: {language_model_inputs.shape}')\n",
    "    inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "    expected_device = language_model_attention_mask.device\n",
    "    attention_mask = torch.cat([language_model_attention_mask, attention_mask.to(expected_device)], dim=1)\n",
    "\n",
    "    if model.config.use_decoder_only_language_model:\n",
    "        outputs = model.language_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        logits = outputs.logits if return_dict else outputs[0]\n",
    "        loss = None\n",
    "        # we compute the loss here since we need to take into account the sequence length of the query embeds\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            logits = logits[:, -labels.size(1) :, :]\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous().to(logits.device)\n",
    "\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss(reduction=\"mean\")\n",
    "            print(f'Calculating loss with shift_logits {shift_logits.shape} and {shift_labels.shape}')\n",
    "            loss = loss_fct(shift_logits.view(-1, model.config.text_config.vocab_size), shift_labels.view(-1))\n",
    "    else:\n",
    "        outputs = model.language_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs.loss if return_dict else outputs[0]\n",
    "        logits = outputs.logits if return_dict else outputs[1]\n",
    "\n",
    "    # if not return_dict:\n",
    "    #     output = (logits, vision_outputs, query_outputs, outputs)\n",
    "    #     return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'logits': logits,\n",
    "        'vision_outputs': vision_outputs,\n",
    "        'qformer_outputs': query_outputs,\n",
    "        'language_model_outputs': outputs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf3c9f-db71-4f7f-a213-e641c8af8fc3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Need loss\n",
    "- How to\n",
    "Read in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ec31f-d9f2-4917-89a8-a64bd5e325cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/full_ds.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04767f53-2e47-4c6e-9662-2bd3b2e24cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = df['comment'].astype(str).apply(lambda str: str[:500]).apply(lambda str: processor.tokenizer.encode(str, padding=\"max_length\",max_length=512))\n",
    "input_ids = torch.tensor(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8c2ab-1614-4b60-bed9-bc4bdfe5b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d05330c-d15b-4cce-b67d-6cd480b7539b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_batch(comments):\n",
    "    xb = [] # B,T,C --> [1, 512, ?]\n",
    "    yb = [] # B,T,1 \n",
    "\n",
    "    for comment in comments:\n",
    "        context = []\n",
    "        \n",
    "        # tokenize the comment\n",
    "        print(comment)\n",
    "        tokenized = processor.tokenizer.encode(comment, padding=\"max_length\", max_length=512)\n",
    "        print(tokenized)\n",
    "        \n",
    "        # for token\n",
    "        for idx, t in enumerate(tokenized):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "                \n",
    "            context = tokenized[0:idx]\n",
    "            label = tokenized[idx:idx + 1]\n",
    "\n",
    "            xb.append(context)\n",
    "            yb.append(label)\n",
    "            \n",
    "    return xb, yb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe377ce-4fc6-4949-a903-386ab827174a",
   "metadata": {},
   "source": [
    "# JUST DO IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc3bf441-c4a9-48b1-9564-6bbb5f026177",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e7f47cf-9471-493c-91fb-b3ad2a7dfa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>image_url</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amnyr9</td>\n",
       "      <td>efnhj58</td>\n",
       "      <td>Sometimes the line between life and suicide is...</td>\n",
       "      <td>1960</td>\n",
       "      <td>4032</td>\n",
       "      <td>https://i.redd.it/s2fi2wr7ibe21.jpg</td>\n",
       "      <td>./images/amnyr9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amnyr9</td>\n",
       "      <td>efngbk8</td>\n",
       "      <td>Your support is amazing, I feel like I can act...</td>\n",
       "      <td>1960</td>\n",
       "      <td>4032</td>\n",
       "      <td>https://i.redd.it/s2fi2wr7ibe21.jpg</td>\n",
       "      <td>./images/amnyr9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amnyr9</td>\n",
       "      <td>efnhf5p</td>\n",
       "      <td>As a Russian are you obligated to dress as the...</td>\n",
       "      <td>1960</td>\n",
       "      <td>4032</td>\n",
       "      <td>https://i.redd.it/s2fi2wr7ibe21.jpg</td>\n",
       "      <td>./images/amnyr9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amnyr9</td>\n",
       "      <td>efngwcp</td>\n",
       "      <td>You're a perfectly valid person and we care ab...</td>\n",
       "      <td>1960</td>\n",
       "      <td>4032</td>\n",
       "      <td>https://i.redd.it/s2fi2wr7ibe21.jpg</td>\n",
       "      <td>./images/amnyr9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amnyr9</td>\n",
       "      <td>efngp00</td>\n",
       "      <td>I had the same haircut once, I was about 1s old</td>\n",
       "      <td>1960</td>\n",
       "      <td>4032</td>\n",
       "      <td>https://i.redd.it/s2fi2wr7ibe21.jpg</td>\n",
       "      <td>./images/amnyr9.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id comment_id                                            comment  \\\n",
       "0        amnyr9    efnhj58  Sometimes the line between life and suicide is...   \n",
       "1        amnyr9    efngbk8  Your support is amazing, I feel like I can act...   \n",
       "2        amnyr9    efnhf5p  As a Russian are you obligated to dress as the...   \n",
       "3        amnyr9    efngwcp  You're a perfectly valid person and we care ab...   \n",
       "4        amnyr9    efngp00   I had the same haircut once, I was about 1s old    \n",
       "\n",
       "   width  height                            image_url           image_path  \n",
       "0   1960    4032  https://i.redd.it/s2fi2wr7ibe21.jpg  ./images/amnyr9.jpg  \n",
       "1   1960    4032  https://i.redd.it/s2fi2wr7ibe21.jpg  ./images/amnyr9.jpg  \n",
       "2   1960    4032  https://i.redd.it/s2fi2wr7ibe21.jpg  ./images/amnyr9.jpg  \n",
       "3   1960    4032  https://i.redd.it/s2fi2wr7ibe21.jpg  ./images/amnyr9.jpg  \n",
       "4   1960    4032  https://i.redd.it/s2fi2wr7ibe21.jpg  ./images/amnyr9.jpg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../datasets/full_ds.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e63fe626-1cf3-49d4-9d3e-2ae8e04c49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass 6 prompts and 6 labels\n",
      "torch.Size([6, 512])\n",
      "torch.Size([6, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n",
      "Could not load library libcudnn_cnn_train.so.8. Error: /usr/local/cuda/lib64/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn5infer22queryClusterPropertiesERPhS3_, version libcudnn_cnn_infer.so.8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GET was unable to find an engine to execute this computation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# TODO: Backprop\u001b[39;00m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/coffeebot-p3lKt8zM-py3.10/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/coffeebot-p3lKt8zM-py3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: GET was unable to find an engine to execute this computation"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "prompt = \"Question: How would you describe this person? Answer:\"\n",
    "labels = []\n",
    "image = None\n",
    "submission_id = None\n",
    "output = None\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    comment = row['comment']\n",
    "    image_url = row['image_url']\n",
    "    \n",
    "    if submission_id is None:\n",
    "        submission_id = row['submission_id']\n",
    "\n",
    "    if image is None:\n",
    "        image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "    # We reached a new submission\n",
    "    if row['submission_id'] != submission_id or len(labels) >5:\n",
    "        # Expand the inputs\n",
    "        prompts = [prompt] * len(labels)\n",
    "        image_inputs = [image] * len(labels)\n",
    "        \n",
    "        # Forward Pass \n",
    "        print(f'Forward pass {len(prompts)} prompts and {len(labels)} labels')\n",
    "        label_input_ids = processor.tokenizer(labels, return_tensors=\"pt\", padding=\"max_length\", max_length=512).input_ids\n",
    "        inputs = processor(image_inputs, prompts, return_tensors=\"pt\", padding=\"max_length\", max_length=512).to(0, torch.float16)\n",
    "        print(label_input_ids.shape)\n",
    "        print(inputs.input_ids.shape)\n",
    "        out = model(pixel_values=inputs.pixel_values, input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=label_input_ids)\n",
    "\n",
    "        # TODO: Backprop\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Loss at step {idx} = {loss}\")\n",
    "        break\n",
    "\n",
    "        # Reset\n",
    "        submission_id = row['submission_id']\n",
    "        image = Image.open(requests.get(row['image_url'], stream=True).raw)\n",
    "        labels = [row['comment']]\n",
    "    else:\n",
    "        labels.append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be608b2f-27e1-4c9f-895a-d2fdc30c818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb  2 09:07:47 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   28C    P0    48W / 400W |  35605MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    313507      C   ...3lKt8zM-py3.10/bin/python    35602MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c180bec-ccc7-4e0b-8dc1-9089f5c592a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350304f-60d5-452b-af45-7fee1837b713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
