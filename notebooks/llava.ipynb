{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47656296-dfdc-4e99-9849-80e5ee7c7082",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf9fccd-d31a-4cab-9b5f-bda5f1428473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoProcessor, LlavaForConditionalGeneration, Blip2Model, Blip2Processor, Blip2ForConditionalGeneration, Blip2Config\n",
    "import time\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11913884-90c6-4233-9609-f9a8b6e1a658",
   "metadata": {},
   "source": [
    "# Load Model Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06824f3c-3586-4c60-84c1-e1323910d2de",
   "metadata": {},
   "source": [
    "## LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7a25a-4e5f-4cee-a66c-b4b692b30d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True,).to(0)\n",
    "# processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ca617-640b-4952-b30d-d2c413d05da7",
   "metadata": {},
   "source": [
    "## BLIP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d804e-6bf2-45e2-9fa1-d56a0154336e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download BLIP2 weights\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d361039-bc38-4f8f-8e98-2530e4acd478",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc909af5-7e98-4927-823b-0bc0ee17aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<image>\\nUSER: What is in the image?\\nASSISTANT:\"\n",
    "label_text = '</s> The image is of two cats laying on a couch with remotes on the couch\\n'\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b974c57-a7fe-404c-bab5-904deb2a291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6110c-8b90-45e9-b23a-ac1a4d66c0f8",
   "metadata": {},
   "source": [
    "## One loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d0afc-4689-4033-9b22-a29faee131a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = ['</s> The image is of two cats laying on a couch with remotes on the couch\\n', ' The image is of two cats laying on a couch with remotes on the couch\\n', '</s>The image is of two cats laying on a couch with remotes on the couch\\n', 'Swagger', ' The', '</s> The']\n",
    "\n",
    "for label_text in yb: \n",
    "    inputs = processor(image, prompt, return_tensors=\"pt\").to(0, torch.float16)\n",
    "    label_input_ids = processor.tokenizer.encode(label_text, return_tensors=\"pt\")\n",
    "    print(inputs.input_ids.shape)\n",
    "    generated_ids = model(pixel_values=inputs.pixel_values, input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=label_input_ids)\n",
    "\n",
    "    final_logit_layer = generated_ids.logits[:,-1,:]\n",
    "    max_token_prob = F.softmax(final_logit_layer).argmax()\n",
    "    prediction = processor.batch_decode(max_token_prob.unsqueeze(0), skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    print(f'Prediction {prediction} vs. {label_text} has loss: {generated_ids.loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92982499-f0bd-42eb-bc78-7130831aa282",
   "metadata": {},
   "source": [
    "## Matrix loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca07a04d-a01c-404d-a579-808d132a6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Question: What is in the image? Answer:\"\n",
    "label_texts = [\n",
    "    '</s> The image is of two cats laying on a couch with remotes on the couch\\n',\n",
    "    ' The image is of two cats laying on a couch with remotes on the couch\\n',\n",
    "    'Swagger',\n",
    "    ' The',\n",
    "    '</s> The'\n",
    "][:3]\n",
    "\n",
    "tokens = [processor.tokenizer.encode(s, return_tensors=\"pt\", padding=\"max_length\", max_length=512) for s in label_texts]\n",
    "print(label_texts)\n",
    "print(type(label_texts))\n",
    "# processor.tokenizer.encode(label_texts, return_tensors=\"pt\", padding=\"max_length\", max_length=512)\n",
    "processor.tokenizer(label_texts, return_tensors=\"pt\", padding=\"max_length\", max_length=512).input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614c373-b5e5-4c0f-bc87-2a9126792ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "# !nvidia-smi --gpu-reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065b2ad-be34-47ef-be04-5e22955412b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU\n",
    "torch.cuda.empty_cache()\n",
    "del generated_ids\n",
    "del model\n",
    "del processor\n",
    "del forward_ids\n",
    "del final_layer_logits\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85401876-672b-43d8-aeb3-9c0fe9bcada2",
   "metadata": {},
   "source": [
    "# Test Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0fa79-8bfd-4dfd-b097-aaf5b35bdcef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(0, torch.float16)\n",
    "forward_ids = model(pixel_values=inputs.pixel_values, input_ids=inputs.input_ids, attention_mask=inputs.attention_mask)\n",
    "# generated_ids = model.generate(pixel_values=inputs.pixel_values, input_ids=inputs.input_ids, max_length=200, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10a9f8-b157-4863-83ef-3e5342e5b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer_logits = forward_ids.logits[:, -1, :]\n",
    "probs = F.softmax(final_layer_logits)\n",
    "idx = torch.argmax(probs)\n",
    "processor.decode(idx)\n",
    "\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42327f4-427d-432e-bf03-c22269226928",
   "metadata": {},
   "source": [
    "# Training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8b70a-bb81-444f-b711-93a7f35233cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40206e5-c160-42d2-b1b3-cb921486ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./test_trainer',     # output directory\n",
    "    num_train_epochs=2,              # total # of training epochs\n",
    "    per_device_train_batch_size=4,   # batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f91674-a1e6-46da-9bdc-5075c1b8acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args.num_train_epochs\n",
    "model.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13cdbee-d432-42cf-8800-30fcfaf79632",
   "metadata": {},
   "source": [
    "# TO-DOs\n",
    "\n",
    "- [] Compute Metrics. Are we just using NTP loss? AKA cross entropy\n",
    "- [] Tune/expose hyperparameters for configuration\n",
    "- [] Setup with W&Bs\n",
    "- [] Build training and eval datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c11d915-beff-487b-b340-3d2e623a345f",
   "metadata": {},
   "source": [
    "# W & B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75baaebe-7329-4ca1-8e98-71c3f31e6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already handled\n",
    "# !wandb login\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"coffee-bot\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"architecture\": model.__class__.__name__,\n",
    "        \"dataset\": \"CIFAR-100\",\n",
    "        \"num_train_epoch\": training_args.num_train_epochs,\n",
    "        \"warmup_steps\": training_args.warmup_steps,\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"per_device_eval_batch_size\": training_args.per_device_eval_batch_size,        \n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df16060-e0d0-4551-adca-300e365a41f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54580559-1e86-404e-bff1-e8c578d4b991",
   "metadata": {},
   "source": [
    "# I'M NOT CRAZY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1975797-92a8-4451-b4bc-6728f94190a7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(0, torch.float16)\n",
    "print(inputs.input_ids)\n",
    "\n",
    "test = torch.tensor([[81.0]]).to(0, torch.float16)\n",
    "\n",
    "inputs.input_ids = inputs.input_ids.to(torch.float16)\n",
    "\n",
    "print(test)\n",
    "print(inputs.input_ids)\n",
    "torch.cat((inputs.input_ids, test), dim=1)\n",
    "# torch.cat((inputs.input_ids, test, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94943f38-77c5-4d7e-9fca-10e92236d243",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df715d-53b7-42be-a8c0-f1bba299e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = None\n",
    "idx = None\n",
    "curr_prompt = prompt\n",
    "input_ids = None\n",
    "res_ids = None\n",
    "\n",
    "# While pred != '\\n'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Get image_embedding and project\n",
    "input_ids, language_model_inputs, vision_outputs = image_encoding(model=model, processor=processor, curr_prompt=curr_prompt)\n",
    "\n",
    "while idx != 50118:\n",
    "\n",
    "    # Forward pass\n",
    "    forward_ids = forward_from_image_query_output(model=model, language_model_inputs=language_model_inputs, input_ids=input_ids)\n",
    "\n",
    "    # Get the final layer of logits\n",
    "    final_layer_logits = forward_ids['logits'][:, -1, :]\n",
    "\n",
    "    # Get prediction\n",
    "    probs = F.softmax(final_layer_logits)\n",
    "    idx = torch.argmax(probs)\n",
    "\n",
    "    # Convert to shape\n",
    "    idx = torch.tensor([[idx.item()]]).to('cuda')\n",
    "\n",
    "    if res_ids is None:\n",
    "        res_ids = idx\n",
    "    else:\n",
    "        res_ids = torch.cat([res_ids, idx], dim = 1)\n",
    "        \n",
    "    input_ids = torch.cat([input_ids, idx], dim = 1)\n",
    "    # print(processor.batch_decode(res_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
    "\n",
    "end_time = time.time()\n",
    "print(res_ids)\n",
    "print(f'Took {end_time - start_time} seconds')\n",
    "processor.batch_decode(input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563466df-d00e-4933-ac99-64e6bedc0acf",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cecdfce-7b75-4f47-8170-92de28fe3163",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_from_image_query_output(\n",
    "    model,\n",
    "    language_model_inputs: torch.FloatTensor,\n",
    "    input_ids: torch.FloatTensor,\n",
    "    attention_mask: Optional[torch.LongTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "):\n",
    "    output_attentions = None\n",
    "    output_hidden_states = None\n",
    "    return_dict = model.config.use_return_dict\n",
    "    \n",
    "    # step 3: use the language model, conditioned on the query outputs and the prompt\n",
    "    language_model_attention_mask = torch.ones(\n",
    "        language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n",
    "    )\n",
    "    inputs_embeds = model.language_model.get_input_embeddings()(input_ids)\n",
    "    inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "    expected_device = language_model_attention_mask.device\n",
    "    attention_mask = torch.cat([language_model_attention_mask, attention_mask.to(expected_device)], dim=1)\n",
    "\n",
    "    outputs = model.language_model(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    logits = outputs.logits if return_dict else outputs[0]\n",
    "    loss = None\n",
    "    # we compute the loss here since we need to take into account the sequence length of the query embeds\n",
    "    if labels is not None:\n",
    "        labels = labels.to(logits.device)\n",
    "        logits = logits[:, -labels.size(1) :, :]\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous().to(logits.device)\n",
    "\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "        loss = loss_fct(shift_logits.view(-1, model.config.text_config.vocab_size), shift_labels.view(-1))\n",
    "\n",
    "    if not return_dict:\n",
    "        output = (logits, outputs)\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'logits': logits,\n",
    "        'language_model_outputs': outputs,\n",
    "    }\n",
    "\n",
    "def image_encoding(\n",
    "    model,\n",
    "    processor,\n",
    "    image,\n",
    "    curr_prompt\n",
    "):  \n",
    "    inputs = processor(image, curr_prompt, return_tensors=\"pt\").to(0, torch.float16)\n",
    "    print(inputs)\n",
    "    pixel_values = inputs.pixel_values\n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    decoder_input_ids = None\n",
    "    decoder_attention_mask = None\n",
    "    output_attentions = None\n",
    "    output_hidden_states = None\n",
    "    labels = None\n",
    "    return_dict = model.config.use_return_dict\n",
    "\n",
    "    # step 1: forward the images through the vision encoder,\n",
    "    # to get image embeddings of shape (batch_size, seq_len, hidden_size)\n",
    "    vision_outputs = model.vision_model(\n",
    "        pixel_values=pixel_values,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    image_embeds = vision_outputs[0]\n",
    "    print(f'Image Embedds: {image_embeds.shape}')\n",
    "\n",
    "    # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n",
    "    image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n",
    "\n",
    "    query_tokens = model.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "    query_outputs = model.qformer(\n",
    "        query_embeds=query_tokens,\n",
    "        encoder_hidden_states=image_embeds,\n",
    "        encoder_attention_mask=image_attention_mask,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    query_output = query_outputs[0]\n",
    "\n",
    "    # step 3: use the language model, conditioned on the query outputs and the prompt\n",
    "    return input_ids, model.language_projection(query_output), vision_outputs\n",
    "\n",
    "def custom_forward(\n",
    "    model,\n",
    "    pixel_values: torch.FloatTensor,\n",
    "    input_ids: torch.FloatTensor,\n",
    "    attention_mask: Optional[torch.LongTensor] = None,\n",
    "    decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "    decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    "):\n",
    "    vision_outputs = model.vision_model(\n",
    "        pixel_values=pixel_values,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    image_embeds = vision_outputs[0]\n",
    "    print(f'Image Embeds: {image_embeds.shape}')\n",
    "\n",
    "    # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n",
    "    image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n",
    "\n",
    "    query_tokens = model.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "    query_outputs = model.qformer(\n",
    "        query_embeds=query_tokens,\n",
    "        encoder_hidden_states=image_embeds,\n",
    "        encoder_attention_mask=image_attention_mask,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    query_output = query_outputs[0]\n",
    "\n",
    "    print(f'Query Output: {query_output.shape}')\n",
    "\n",
    "    # step 3: use the language model, conditioned on the query outputs and the prompt\n",
    "    language_model_inputs = model.language_projection(query_output)\n",
    "    language_model_attention_mask = torch.ones(\n",
    "        language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n",
    "    )\n",
    "    print(f'Input Ids: {input_ids.shape}')\n",
    "    inputs_embeds = model.language_model.get_input_embeddings()(input_ids)\n",
    "    print(f'Input Embeds: {inputs_embeds.shape}')\n",
    "    print(f'language_model_inputs: {language_model_inputs.shape}')\n",
    "    inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "    expected_device = language_model_attention_mask.device\n",
    "    attention_mask = torch.cat([language_model_attention_mask, attention_mask.to(expected_device)], dim=1)\n",
    "\n",
    "    if model.config.use_decoder_only_language_model:\n",
    "        outputs = model.language_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        logits = outputs.logits if return_dict else outputs[0]\n",
    "        loss = None\n",
    "        # we compute the loss here since we need to take into account the sequence length of the query embeds\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            logits = logits[:, -labels.size(1) :, :]\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous().to(logits.device)\n",
    "\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss(reduction=\"mean\")\n",
    "            print(f'Calculating loss with shift_logits {shift_logits.shape} and {shift_labels.shape}')\n",
    "            loss = loss_fct(shift_logits.view(-1, model.config.text_config.vocab_size), shift_labels.view(-1))\n",
    "    else:\n",
    "        outputs = model.language_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs.loss if return_dict else outputs[0]\n",
    "        logits = outputs.logits if return_dict else outputs[1]\n",
    "\n",
    "    # if not return_dict:\n",
    "    #     output = (logits, vision_outputs, query_outputs, outputs)\n",
    "    #     return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'logits': logits,\n",
    "        'vision_outputs': vision_outputs,\n",
    "        'qformer_outputs': query_outputs,\n",
    "        'language_model_outputs': outputs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf3c9f-db71-4f7f-a213-e641c8af8fc3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Need loss\n",
    "- How to\n",
    "Read in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ec31f-d9f2-4917-89a8-a64bd5e325cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/full_ds.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04767f53-2e47-4c6e-9662-2bd3b2e24cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = df['comment'].astype(str).apply(lambda str: str[:500]).apply(lambda str: processor.tokenizer.encode(str, padding=\"max_length\",max_length=512))\n",
    "input_ids = torch.tensor(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8c2ab-1614-4b60-bed9-bc4bdfe5b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d05330c-d15b-4cce-b67d-6cd480b7539b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_batch(comments):\n",
    "    xb = [] # B,T,C --> [1, 512, ?]\n",
    "    yb = [] # B,T,1 \n",
    "\n",
    "    for comment in comments:\n",
    "        context = []\n",
    "        \n",
    "        # tokenize the comment\n",
    "        print(comment)\n",
    "        tokenized = processor.tokenizer.encode(comment, padding=\"max_length\", max_length=512)\n",
    "        print(tokenized)\n",
    "        \n",
    "        # for token\n",
    "        for idx, t in enumerate(tokenized):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "                \n",
    "            context = tokenized[0:idx]\n",
    "            label = tokenized[idx:idx + 1]\n",
    "\n",
    "            xb.append(context)\n",
    "            yb.append(label)\n",
    "            \n",
    "    return xb, yb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe377ce-4fc6-4949-a903-386ab827174a",
   "metadata": {},
   "source": [
    "# JUST DO IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "095008a7-10a3-4fab-81f6-ab170f3caf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7530c9860c4956879c3254a430d115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Allocated after instantiating model: 4 GB\n"
     ]
    }
   ],
   "source": [
    "# Download BLIP2 weights\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16)\n",
    "print(f'Memory Allocated after instantiating model: {torch.cuda.memory_allocated(0)/1e9:.4g} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc3bf441-c4a9-48b1-9564-6bbb5f026177",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e7f47cf-9471-493c-91fb-b3ad2a7dfa83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>image_url</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amnyr9</td>\n",
       "      <td>efnhj58</td>\n",
       "      <td>Sometimes the line between life and suicide is...</td>\n",
       "      <td>1960</td>\n",
       "      <td>4032</td>\n",
       "      <td>https://i.redd.it/s2fi2wr7ibe21.jpg</td>\n",
       "      <td>./images/amnyr9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amnyr9</td>\n",
       "      <td>efngbk8</td>\n",
       "      <td>Your support is amazing, I feel like I can act...</td>\n",
       "      <td>1960</td>\n",
       "      <td>4032</td>\n",
       "      <td>https://i.redd.it/s2fi2wr7ibe21.jpg</td>\n",
       "      <td>./images/amnyr9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amnyr9</td>\n",
       "      <td>efnhf5p</td>\n",
       "      <td>As a Russian are you obligated to dress as the...</td>\n",
       "      <td>1960</td>\n",
       "      <td>4032</td>\n",
       "      <td>https://i.redd.it/s2fi2wr7ibe21.jpg</td>\n",
       "      <td>./images/amnyr9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amnyr9</td>\n",
       "      <td>efngwcp</td>\n",
       "      <td>You're a perfectly valid person and we care ab...</td>\n",
       "      <td>1960</td>\n",
       "      <td>4032</td>\n",
       "      <td>https://i.redd.it/s2fi2wr7ibe21.jpg</td>\n",
       "      <td>./images/amnyr9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amnyr9</td>\n",
       "      <td>efngp00</td>\n",
       "      <td>I had the same haircut once, I was about 1s old</td>\n",
       "      <td>1960</td>\n",
       "      <td>4032</td>\n",
       "      <td>https://i.redd.it/s2fi2wr7ibe21.jpg</td>\n",
       "      <td>./images/amnyr9.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id comment_id                                            comment  \\\n",
       "0        amnyr9    efnhj58  Sometimes the line between life and suicide is...   \n",
       "1        amnyr9    efngbk8  Your support is amazing, I feel like I can act...   \n",
       "2        amnyr9    efnhf5p  As a Russian are you obligated to dress as the...   \n",
       "3        amnyr9    efngwcp  You're a perfectly valid person and we care ab...   \n",
       "4        amnyr9    efngp00   I had the same haircut once, I was about 1s old    \n",
       "\n",
       "   width  height                            image_url           image_path  \n",
       "0   1960    4032  https://i.redd.it/s2fi2wr7ibe21.jpg  ./images/amnyr9.jpg  \n",
       "1   1960    4032  https://i.redd.it/s2fi2wr7ibe21.jpg  ./images/amnyr9.jpg  \n",
       "2   1960    4032  https://i.redd.it/s2fi2wr7ibe21.jpg  ./images/amnyr9.jpg  \n",
       "3   1960    4032  https://i.redd.it/s2fi2wr7ibe21.jpg  ./images/amnyr9.jpg  \n",
       "4   1960    4032  https://i.redd.it/s2fi2wr7ibe21.jpg  ./images/amnyr9.jpg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../datasets/full_ds.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63fe626-1cf3-49d4-9d3e-2ae8e04c49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass 6 prompts and 6 labels\n",
      "torch.Size([6, 256])\n",
      "torch.Size([6, 256])\n",
      "Memory Allocated after processing input: 4.001 GB\n",
      "Memory Allocated after foward pass: 17.3 GB\n",
      "Memory Allocated after calculating gradients: 10.29 GB\n",
      "Memory Allocated after updating gradients: 10.84 GB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMemory Allocated after updating gradients: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43midx\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Reset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'idx' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "prompt = \"Question: How would you describe this person? Answer:\"\n",
    "labels = []\n",
    "image = None\n",
    "submission_id = None\n",
    "output = None\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    comment = row['comment']\n",
    "    image_url = row['image_url']\n",
    "    \n",
    "    if submission_id is None:\n",
    "        submission_id = row['submission_id']\n",
    "\n",
    "    if image is None:\n",
    "        image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "    # We reached a new submission\n",
    "    if row['submission_id'] != submission_id or len(labels) >5:\n",
    "        # Expand the inputs\n",
    "        prompts = [prompt] * len(labels)\n",
    "        image_inputs = [image] * len(labels)\n",
    "        \n",
    "        # Forward Pass \n",
    "        print(f'Forward pass {len(prompts)} prompts and {len(labels)} labels')\n",
    "        label_input_ids = processor.tokenizer(labels, return_tensors=\"pt\", padding=\"max_length\", max_length=256).input_ids\n",
    "        inputs = processor(image_inputs, prompts, return_tensors=\"pt\", padding=\"max_length\", max_length=256).to(0, torch.float16)\n",
    "        print(label_input_ids.shape)\n",
    "        print(inputs.input_ids.shape)\n",
    "        print(f'Memory Allocated after processing input: {torch.cuda.memory_allocated(0)/1e9:.4g} GB')\n",
    "        out = model(pixel_values=inputs.pixel_values, input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, labels=label_input_ids)\n",
    "        print(f'Memory Allocated after foward pass: {torch.cuda.memory_allocated(0)/1e9:.4g} GB')\n",
    "\n",
    "        # TODO: Backprop\n",
    "        out.loss.backward()\n",
    "        print(f'Memory Allocated after calculating gradients: {torch.cuda.memory_allocated(0)/1e9:.4g} GB')\n",
    "        optimizer.step()\n",
    "        print(f'Memory Allocated after updating gradients: {torch.cuda.memory_allocated(0)/1e9:.4g} GB')\n",
    "        print(f\"Loss at step {index} = {loss}\")\n",
    "        break\n",
    "\n",
    "        # Reset\n",
    "        submission_id = row['submission_id']\n",
    "        image = Image.open(requests.get(row['image_url'], stream=True).raw)\n",
    "        labels = [row['comment']]\n",
    "    else:\n",
    "        labels.append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad406c8-ebd2-4fba-b315-2ff6460bb11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.2.0+cu121\n",
      "Torch available: True\n",
      "CUDA version: 12.1\n",
      "CUDNN Version: 8900\n",
      "CUDNN Available: True\n"
     ]
    }
   ],
   "source": [
    "print(f'Torch Version: {torch.__version__}')\n",
    "print(f'Torch available: {torch.cuda.is_available()}')\n",
    "print(f'CUDA version: {torch.version.cuda}')\n",
    "print(f'CUDNN Version: {torch.backends.cudnn.version()}')\n",
    "print(f'CUDNN Available: {torch.backends.cudnn.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee2e383-e93b-4d3b-95e3-c95a7267e0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2258,  0.5908,  0.5395,  ..., -0.4635, -0.0105,  1.0169],\n",
      "          [-0.0187,  0.9595,  0.0972,  ...,  0.1228,  0.2397,  0.9744],\n",
      "          [ 1.2488,  0.2947, -0.5481,  ..., -0.9220, -0.3484, -0.1615],\n",
      "          ...,\n",
      "          [-0.5578,  0.1989, -0.7338,  ..., -0.4295, -0.6536,  0.0914],\n",
      "          [ 0.0763,  0.5592, -0.6916,  ...,  0.6397,  0.8285,  0.4860],\n",
      "          [ 0.1623, -0.1973, -1.0511,  ...,  0.6243,  0.3905,  0.0339]],\n",
      "\n",
      "         [[ 0.1889, -0.2638,  0.1338,  ...,  0.5188, -0.2363,  0.5212],\n",
      "          [ 0.1145, -0.3007,  0.5670,  ...,  0.4922,  0.7508, -1.2253],\n",
      "          [ 0.6015,  0.1288, -0.6906,  ..., -0.1090, -0.0872, -0.2802],\n",
      "          ...,\n",
      "          [-0.2399,  0.3944,  1.0016,  ...,  0.4502,  0.2785,  0.2913],\n",
      "          [ 0.5198, -1.0184,  0.0961,  ..., -0.1031,  1.1517, -0.1195],\n",
      "          [-0.5453, -0.1497,  1.2029,  ..., -0.1827,  0.6232,  0.2847]],\n",
      "\n",
      "         [[ 0.6258,  0.2314,  0.1132,  ...,  0.2032, -1.3933, -1.0281],\n",
      "          [-0.3209, -1.3799, -0.0428,  ...,  0.4349,  0.2951, -0.0609],\n",
      "          [-0.1250,  0.2409, -0.1975,  ..., -1.2700, -0.3357, -0.5626],\n",
      "          ...,\n",
      "          [ 0.1862, -1.0234,  0.0721,  ...,  0.2698, -0.1130, -0.1836],\n",
      "          [-1.0144,  0.1447,  0.1881,  ..., -0.4207,  1.0672, -0.4738],\n",
      "          [ 0.0108, -0.5157, -0.5027,  ..., -0.2091, -0.1654,  0.3841]]]],\n",
      "       device='cuda:0', grad_fn=<ConvolutionBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8900"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Works\n",
    "x = torch.randn(1, 3, 224, 224).cuda()\n",
    "conv = torch.nn.Conv2d(3, 3, 3).cuda()\n",
    "out = conv(x)\n",
    "print(out)\n",
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d3823fe-740d-4665-bfe5-2b8605a648d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/devonperoutky/.cache/pypoetry/virtualenvs/coffeebot-p3lKt8zM-py3.10/lib/python3.10/site-packages/torch']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872cbb4-f283-4f8b-9e79-5a3a9fe485bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
