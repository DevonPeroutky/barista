{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dbc9c83-5b47-4eee-8c3f-7f73dbbc9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"ybelkada/football-dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5876322-3fa1-4672-93ee-61fbdb1168ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        encoding = self.processor(images=item[\"image\"], text=item[\"text\"], padding=\"max_length\", max_length=75, return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba9ac31-44b8-4ae7-b168-cab7d775e708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282e9025ac974a8e91b075e1741482d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, BlipForConditionalGeneration, Blip2ForConditionalGeneration\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", device_map={\"\": 0}).cuda()\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", device_map={\"\": 0}).cuda()\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map={\"\": 0}, torch_dtype=torch.float16).cuda()\n",
    "# model2 = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map={\"\": 0}).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5e2987f-cf9f-408a-bccd-f34d66722db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Allocated after instantiating model: 22.66 GB\n"
     ]
    }
   ],
   "source": [
    "print(f'Memory Allocated after instantiating model: {torch.cuda.memory_allocated(0)/1e9:.4g} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac948a0c-f9d5-4ce5-8c71-206c46d7c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptioningDataset(dataset, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f088f8-0369-41ba-8867-118abc42535e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f720ca7-eaab-438d-a0d0-a4cf6600221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 0.0251312255859375\n",
      "Loss: 0.08270263671875\n",
      "Loss: 0.05401611328125\n",
      "Loss: 0.045013427734375\n",
      "Loss: 0.0472412109375\n",
      "Loss: 0.0477294921875\n",
      "Epoch: 1\n",
      "Loss: 0.022186279296875\n",
      "Loss: 0.034271240234375\n",
      "Loss: 0.046417236328125\n",
      "Loss: 0.039825439453125\n",
      "Loss: 0.04345703125\n",
      "Loss: 0.0352783203125\n",
      "Epoch: 2\n",
      "Loss: 0.0207672119140625\n",
      "Loss: 0.028076171875\n",
      "Loss: 0.03887939453125\n",
      "Loss: 0.037933349609375\n",
      "Loss: 0.031829833984375\n",
      "Loss: 0.047576904296875\n",
      "Epoch: 3\n",
      "Loss: 0.0462646484375\n",
      "Loss: 0.03204345703125\n",
      "Loss: 0.0261383056640625\n",
      "Loss: 0.03436279296875\n",
      "Loss: 0.044769287109375\n",
      "Loss: 0.028656005859375\n",
      "Epoch: 4\n",
      "Loss: 0.0228118896484375\n",
      "Loss: 0.0272674560546875\n",
      "Loss: 0.039031982421875\n",
      "Loss: 0.0362548828125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     17\u001b[0m                 pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[1;32m     18\u001b[0m                 labels\u001b[38;5;241m=\u001b[39minput_ids)    \n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# ADDRESS GRADIENTS EXPLOSION  \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# scale_factor = 0.001\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# for param in model.parameters():\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# clip_grad_norm_(model.parameters(), 1.0)\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/coffeebot-p3lKt8zM-py3.10/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/coffeebot-p3lKt8zM-py3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, eps=10e-4)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(50):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch.pop(\"input_ids\").to(device)\n",
    "    pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=input_ids)    \n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "    # ADDRESS GRADIENTS EXPLOSION  \n",
    "    # scale_factor = 0.001\n",
    "    # for param in model.parameters():\n",
    "    #     param.grad *= scale_factor\n",
    "\n",
    "    # Clamp Gradients\n",
    "    # for param in model.parameters():\n",
    "    #     param.grad = torch.clamp(param.grad, min=.001, max=.99)\n",
    "        \n",
    "    # clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284f54e-c592-4d45-a7d1-521d52a1ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_csv = '../datasets/full_ds.csv'\n",
    "dataset = load_dataset(\"csv\", data_files=my_csv)\n",
    "\n",
    "# df = pd.read_csv('../datasets/full_ds.csv')\n",
    "# df.head()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3306550-4c01-42d3-8c01-84ad960771c3",
   "metadata": {},
   "source": [
    "# PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4331cc-2e89-41ea-9d77-9291b9ec209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_weights_histogram(model, layer_names=None):\n",
    "    for name, param in model.named_parameters():\n",
    "        # if layer_names is None or any(layer_name in name for layer_name in layer_names):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(param.data.flatten().cpu().numpy(), bins=50, color='blue', alpha=0.7)\n",
    "        plt.title(f'Weight Distribution - {name}')\n",
    "        plt.xlabel('Weight Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "plot_weights_histogram(model, layer_names=['fc', 'conv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba5f1f-1b12-438d-baa2-cd2bd3cca280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "def plot_weights_outliers(model, layer_names=None, threshold=3):\n",
    "    outlier_data = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if layer_names is None or any(layer_name in name for layer_name in layer_names):\n",
    "            # Flatten and convert to numpy array\n",
    "            weights = param.data.flatten().cpu().numpy()\n",
    "\n",
    "            # Detect outliers using IQR method\n",
    "            q25, q75 = torch.percentile(param.data, [25, 75])\n",
    "            iqr = q75 - q25\n",
    "            lower_bound = q25 - threshold * iqr\n",
    "            upper_bound = q75 + threshold * iqr\n",
    "\n",
    "            # Identify outliers\n",
    "            outliers = weights[(weights < lower_bound) | (weights > upper_bound)]\n",
    "\n",
    "            # Store layer information and outliers\n",
    "            layer_info = {'Layer': name, 'Outliers': outliers}\n",
    "            outlier_data.append(layer_info)\n",
    "\n",
    "    # Plot outliers using box plots\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x='Layer', y='Outliers', data=outlier_data)\n",
    "    plt.title('Outliers in Weights Across Layers')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'your_model' is an instance of your neural network\n",
    "plot_weights_outliers(your_model, layer_names=['fc', 'conv'], threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f130376-800c-43a9-9c38-045c1ccb66d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
