{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e88c62dc-7bb7-4791-9859-f01e66c98340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, Blip2Model, Blip2Processor, Blip2ForConditionalGeneration, Blip2Config\n",
    "import time\n",
    "import torch\n",
    "import accelerate\n",
    "from torch.nn import functional as F\n",
    "from typing import Any, Optional, Tuple, Union\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb13e43-cc63-47e6-b785-db74bfa0c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download BLIP2 weights\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16)\n",
    "# model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d966fa42-b75f-4c70-879a-f85b1a1e1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6b7db61-bfd2-4bb2-afd7-72bef5ecb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_from_image_query_output(\n",
    "    model,\n",
    "    language_model_inputs: torch.FloatTensor,\n",
    "    input_ids: torch.FloatTensor,\n",
    "    attention_mask: Optional[torch.LongTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "):\n",
    "    output_attentions = None\n",
    "    output_hidden_states = None\n",
    "    return_dict = model.config.use_return_dict\n",
    "    \n",
    "    # step 3: use the language model, conditioned on the query outputs and the prompt\n",
    "    language_model_attention_mask = torch.ones(\n",
    "        language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n",
    "    )\n",
    "    inputs_embeds = model.language_model.get_input_embeddings()(input_ids)\n",
    "    inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "    expected_device = language_model_attention_mask.device\n",
    "    attention_mask = torch.cat([language_model_attention_mask, attention_mask.to(expected_device)], dim=1)\n",
    "\n",
    "    outputs = model.language_model(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    logits = outputs.logits if return_dict else outputs[0]\n",
    "    loss = None\n",
    "    # we compute the loss here since we need to take into account the sequence length of the query embeds\n",
    "    if labels is not None:\n",
    "        labels = labels.to(logits.device)\n",
    "        logits = logits[:, -labels.size(1) :, :]\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous().to(logits.device)\n",
    "\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "        loss = loss_fct(shift_logits.view(-1, model.config.text_config.vocab_size), shift_labels.view(-1))\n",
    "\n",
    "    if not return_dict:\n",
    "        output = (logits, outputs)\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'logits': logits,\n",
    "        'language_model_outputs': outputs,\n",
    "    }\n",
    "\n",
    "def image_encoding(\n",
    "    model,\n",
    "    processor,\n",
    "    prompt,\n",
    "):\n",
    "    inputs = processor(image, curr_prompt, return_tensors=\"pt\").to(0, torch.float16)\n",
    "    pixel_values = inputs.pixel_values\n",
    "\n",
    "    decoder_input_ids = None\n",
    "    decoder_attention_mask = None\n",
    "    output_attentions = None\n",
    "    output_hidden_states = None\n",
    "    labels = None\n",
    "    return_dict = model.config.use_return_dict\n",
    "\n",
    "    # step 1: forward the images through the vision encoder,\n",
    "    # to get image embeddings of shape (batch_size, seq_len, hidden_size)\n",
    "    vision_outputs = model.vision_model(\n",
    "        pixel_values=pixel_values,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    image_embeds = vision_outputs[0]\n",
    "    print(f'Image Embedds: {image_embeds.shape}')\n",
    "\n",
    "    # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n",
    "    image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n",
    "\n",
    "    query_tokens = model.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "    query_outputs = model.qformer(\n",
    "        query_embeds=query_tokens,\n",
    "        encoder_hidden_states=image_embeds,\n",
    "        encoder_attention_mask=image_attention_mask,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    query_output = query_outputs[0]\n",
    "\n",
    "    # step 3: use the language model, conditioned on the query outputs and the prompt\n",
    "    return model.language_projection(query_output), vision_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9916b32d-8564-4dde-a444-c7220f1ffa84",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4398bcf4-8f6d-456c-8343-38a23c9e9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = '../datasets/full_ds.csv'\n",
    "BATCH_SIZE = 2\n",
    "BLOCK_SIZE = 250\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f947e1cd-3190-481c-8218-02af5135d9d1",
   "metadata": {},
   "source": [
    "#  Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d03d0-d8e3-4c28-9d1e-c5ad865c620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset[\"train\"], batch_size= BATCH_SIZE)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "for _ in range(epochs):\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(text = batch[\"input_ids\"], image = batch[\"image\"], labels = batch[\"answers\"])\n",
    "    loss = out.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Loss at step {idx} = {loss}\")\n",
    "    if idx >= MAX_STEPS-1:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad7a21-2326-4d36-8ecc-b8ba5d866404",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torch.tensor([]) # [B,T,C]\n",
    "\n",
    "for _ in range(epochs):\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Input_IDS\n",
    "    input_ids = batch[\"input_ids\"] # [T,C]\n",
    "    pixel_ids = batch[\"pixel_ids\"] # [T,C]\n",
    "    labels = batch[\"labels\"] # [T,1]\n",
    "    \n",
    "    \n",
    "    out = model(text = input_ids, image = pixel_ids, labels = labels)\n",
    "    loss = out.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Loss at step {idx} = {loss}\")\n",
    "    if idx >= MAX_STEPS-1:\n",
    "      break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
