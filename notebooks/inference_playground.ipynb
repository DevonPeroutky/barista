{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff95b7-ccf4-4e90-b61f-1b12f336f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from transformers import BlipProcessor, Blip2Processor, Blip2ForConditionalGeneration, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d28e219-9358-4b00-8b7a-bd650d76e2cf",
   "metadata": {},
   "source": [
    "# Steps\n",
    "1. Load the dataset (Use HF data loaders or write own)\n",
    "2. Pick base model(s) to test\n",
    "    3. BLIP-2, LLAVA, BLIP\n",
    "4. Run some example inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf5d06e-c850-4496-87e4-48a2298d78d3",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bde567-46e4-47ff-bb82-2933155ff188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download BLIP2 weights\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3842151-2a21-430a-8dc5-3fe5a8a7161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e9710-e280-4651-a4f9-a52e31249921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download BLIP weights\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ad8d3-feed-43ca-b9bc-404234f18539",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80186f6-0d8c-459e-8c0e-05f570fc0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download nlpconnect/vit-gpt2-image-captioning\n",
    "# image_to_text = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "del image_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf803e-a6ce-460e-a112-c33e9af1a97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test image\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "img_url = 'https://i.redd.it/s2fi2wr7ibe21.jpg'\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "display(raw_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d23ee3-4e38-46b2-a16b-398c07daea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"A photograph of \"\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c086496-ebc2-4d43-a296-de7b3523ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db1aa9-3d16-44e8-bc0e-b7803b9accd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(**inputs)\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71571b6f-2cee-4c16-8063-ebe704026805",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processor.decode(out[0], skip_special_tokens=False).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231bcde3-8ee9-43d3-9af7-9685e21edd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processor.decode(inputs['input_ids'][0], skip_special_tokens=False).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e352468-ade3-4505-8819-1782ebf13a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.decode([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7126b-4575-4a10-9cf9-e753b907c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d9867-ef6a-425d-b47b-53d5faf395f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids'] = torch.tensor([[   0, 9178,  171, 3678,   32,   11,    5, 2170,  116, 2]])\n",
    "inputs['attention_mask'] = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f85de5-db2e-467f-88e7-7715c8dcf39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_text(img_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
